\documentclass{scrartcl}

\usepackage{csquotes}
% Babel provides hyphenation patterns and translations of keywords like 'table
% of contents'
\usepackage[ngerman]{babel}
% Provides commands for type-setting mathematical formulas
\usepackage{amsmath}
% Provides additional symbols
\usepackage{amssymb}
% Provides environments for typical math text structure
\usepackage{amsthm}
\usepackage{biblatex}
% Automatic generation of hyperlinks for references and URIs
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{float}

\KOMAoptions{
  % Add vertical space between two paragraphs, without indent
  parskip=true,
}

\subject{Bericht Handout}
\titlehead{%
  \begin{minipage}{.7\textwidth}%
  Humboldt-Universit\"at zu Berlin\\
  Mathematisch-Naturwissenschaftliche Fakult\"at\\
  Institut f\"ur Mathematik
  \end{minipage}
}
\title{Poisson-Problem}
\author{%
  Eingereicht von M. van Straten und P. Merz
}
\date{\today}

\newcommand\N{\mathbb{N}}
\newcommand\R{\mathbb{R}}
\newcommand\BigO{\mathcal{O}}

\DeclarePairedDelimiterXPP{\set}[1]{}{\lbrace}{\rbrace}{}{#1}
\DeclarePairedDelimiterXPP{\paren}[1]{}{\lparen}{\rparen}{}{#1}

\graphicspath{{./figures/}}

\addbibresource{handout.bib}

\begin{document}

\maketitle
\tableofcontents
\cleardoublepage%

\section{Einleitung und Motivation}

Die Poisson-Gleichung, benannt nach dem französischen Mathematiker und Physiker
Siméon Denis Poisson, ist eine elliptische partielle Differenzialgleichung mit
zentraler Bedeutung in zahlreichen Bereichen der Ingenieurwissenschaften und
Physik. Beispiele hierfür sind die Gravitationstheorie, Elektrostatik und
Strömungsmechanik\ \cite{Poisson}. Da analytische Lösungen solcher
Differenzialgleichungen oft äußerst anspruchsvoll oder gar unmöglich zu finden
sind, bietet sich die numerische Approximation als praktikable Alternative an.
Im Rahmen der bisherigen Aufgabenreihe wurden bereits Verfahren zur
Approximation von Ableitungen erster und zweiter Ordnung untersucht. Aufbauend
darauf soll in dieser Arbeit für gegebene Randbedingungen eine numerische
Lösung der Poisson-Gleichung ermittelt und mit der analytischen Lösung
verglichen werden.

\section{Theorie}

\subsection{Poisson-Problem}

Das Poisson-Problem besteht in der Bestimmung einer Funktion \(u \in C^2(\R^2;
\R)\), die für ein gegebenes Gebiet \(\Omega \subset \R^2\) mit Rand \(\partial
\Omega\) sowie zwei vorgegebene Funktionen \(f \in C(\Omega; \R)\) und \(g \in
C(\partial \Omega; \R)\) die folgenden Bedingungen erfüllt:
\begin{align*}
    -\Delta u & = f \; \text{in} \; \Omega           \\
    u         & = g \; \text{auf} \; \partial \Omega
\end{align*}
wobei
\begin{equation*}
    \Delta u = \frac{\partial^2 u}{\partial x_1^2} + \frac{\partial^2 u}{\partial x_2^2}
\end{equation*}
den Laplace-Operator von \(u\) darstellt\ \cite{PPI_Poisson}.

In diesem Handout wird speziell der Fall
\[
    \Omega = {(0, 1)}^2 \quad \text{und} \quad g = 0
\] behandelt.

\subsection{Diskretisierung des Gebietes}

Zur Diskretisierung des zweidimensionalen Gebietes \(\Omega = {(0,1)}^2\) wird
dieses in ein äquidistantes Gitter von Punkten unterteilt. Dazu wird das
Intervall \((0,1)\) in \(n\) gleich lange Teilintervalle der Länge \(1 / n\)
unterteilt, was zur Menge der inneren Gitterpunkte entlang der ersten
Koordinate
\[
    X_1 = \set*{\frac{j}{n} \mid 1 \leqslant j \leqslant n - 1}
\]
führt. Die Gesamtheit der Diskretisierungspunkte im zweidimensionalen Gebiet
  ergibt sich durch das kartesische Produkt der einzelnen Koordinatenmengen:
\[
    X
    = X_1 \times X_1
    = \set*{\paren*{\frac{j}{n}, \frac{k}{n}} \mid 1 \leqslant j, k \leqslant n - 1}.
\]
Diese Diskretisierung wird im Rahmen der numerischen Lösung des
  Poisson-Problems verwendet, wie in~\cite{PPI_Poisson} beschrieben.

\subsection{Diskretisierung des Laplace-Operators}

Mithilfe der Methode der finiten Differenzen, die bereits im ersten Abgabeteil
behandelt wurden, können die zweiten partiellen Ableitungen einer Funktion nach
den Variablen \(x_1\) und \(x_2\) wie folgt approximiert werden:
\[
    \frac{\partial^2 u}{\partial x_1^2} (v, w) \approx \frac{u(v + h, w) - 2u(v, w) + u(v - h, w)}{h^2}
\]
sowie
\[
    \frac{\partial^2 u}{\partial x_2^2} (v, w) \approx \frac{u(v, w + h) - 2u(v, w) + u(v, w - h)}{h^2},
\]
wobei \(h = 1 / n\) für \(n \in \N^+\) gilt und \((v, w) \in X\).

Aus diesen Approximationen ergibt sich für den Laplace-Operator die folgende
diskrete Darstellung:
\begin{align*}
    \Delta u & = \frac{\partial^2 u}{\partial x_1^2} + \frac{\partial^2 u}{\partial x_2^2}           \\
             & \approx \frac{u(v + h, w) + u(v, w + h) - 4u(v, w) + u(v - h, w) + u(v, w - h)}{h^2}.
\end{align*}

Dieser diskrete Laplace-Operator wird mit \(\Delta_h u\) bezeichnet.

\subsection{Aufstellen des linearen Gleichungssystems}

Ziel ist es, eine Lösung \(\hat u\) der diskretisierten partiellen
Differenzialgleichung zu finden, die das folgende Problem löst:
\begin{align*}
    -\Delta_{h} u(x) & = f \quad \text{für} \quad x \in X,         \\
    u(x)             & = 0 \quad \text{auf} \quad \partial \Omega.
\end{align*}
Dieses Problem wird an den \(N = {(n-1)}^2\) inneren Gitterpunkten des
Diskretisierungsgebiets \(X\) formuliert, wodurch sich ein System von \(N\)
linearen Gleichungen ergibt, das gelöst werden muss.

Zur numerischen Behandlung dieses Systems wird eine Ordnung der Gitterpunkte
definiert. Für \(x = (x_1, x_2)\) und \(y = (y_1, y_2) \in X\) wird die
folgende lexikografische Ordnung eingeführt:
\[
    x <_{X} y \iff x_1 + x_2n < y_1 + y_2n.
\]
Diese Ordnung induziert eine Bijektion zwischen den Gitterpunkten und den
  Gleichungsindizes:
\begin{align*}
    \operatorname{idx}: \set{1, \ldots, n - 1}^2 & \longrightarrow \set*{1, \ldots , N}, \\
    (j, k)                                       & \longmapsto (k - 1)(n - 1) + j.
\end{align*}
Durch diese Abbildung wird jedem Diskretisierungspunkt \((j, k)\) eine eindeutige
Gleichungsnummer zugewiesen.

Das resultierende lineare Gleichungssystem wird durch die Matrix \(h^{-2}A\)
beschrieben, wobei \(A \in \R^{N \times N}\) eine blockdiagonale Struktur
aufweist:
\[
    A \coloneq \begin{bmatrix}
        C      & -I     & 0      & \cdots & 0      \\
        -I     & C      & -I     & \cdots & 0      \\
        0      & \ddots & \ddots & \ddots & \vdots \\
        \vdots & \ddots & \ddots & \ddots & -I     \\
        0      & \cdots & 0      & -I     & C
    \end{bmatrix},
\]
wobei \(C \in \R^{(n-1) \times (n-1)}\) eine Tridiagonalmatrix ist:
\[
    C \coloneq \begin{bmatrix}
        4      & -1     & 0      & \cdots & 0      \\
        -1     & 4      & -1     & \cdots & 0      \\
        0      & \ddots & \ddots & \ddots & \vdots \\
        \vdots & \ddots & \ddots & \ddots & -1     \\
        0      & \cdots & 0      & -1     & 4
    \end{bmatrix}.
\]
Diese Matrixstruktur ergibt sich aus der Diskretisierung des
  Laplace-Operators und der gewählten Ordnung der
  Gitterpunkte~\cite{PPI_Poisson}.

\subsection{LU-Zerlegung einer Matrix}

Sei \(A \in \R^{n \times n}\) eine quadratische Matrix. Dann existieren eine
Permutationsmatrix \(P \in \R^{n \times n}\), eine linke untere Dreiecksmatrix
\(L \in \R^{n \times n}\) und eine rechte obere Dreiecksmatrix \(U \in \R^{n
\times n}\), sodass gilt:
\[
    A = PLU.
\]
Die Permutationsmatrix \(P\) erfüllt \(P^{-1} = P^T\), da sie eine
  Permutationsmatrix ist~\cite{LU}. Liegt nun ein lineares Gleichungssystem der
  Form \(Ax = b\) vor, so lässt sich dieses wie folgt umformulieren:
\[
    Ax = b \iff PLUx = b \iff LUx = P^T b.
\]
Mit der Substitution \(Ux = z\) kann das Gleichungssystem \(Lz = P^T b\)
  mittels vorwärts Substitution in \(\BigO(n^2)\) gelöst werden. Anschließend
  kann das Gleichungssystem \(Ux = z\) mittels rückwärts Substitution ebenfalls
  in \(\BigO(n^2)\) gelöst werden. Dies liefert die Lösung \(x\) des
  ursprünglichen Gleichungssystems \(Ax = b\).

\subsection{Vergleich: A als vollbesetzte und sparse Matrix}

Die Matrix \(A\) weist eine signifikante Anzahl von Nulleinträgen auf. Für
größere Werte von \(n\) wird das Speichern der Matrix \(A\) als vollbesetzte
Matrix sehr ineffizient. Daher ist die Verwendung von Sparse-Matrizen
unerlässlich. Sparse-Matrizen speichern lediglich die von null verschiedenen
Einträge zusammen mit ihren Koordinaten innerhalb der Matrix. Für jeden
nicht-Nulleintrag werden somit drei Werte gespeichert: der Eintrag selbst sowie
seine Zeilen- und Spaltenindizes.

Die Anzahl der Einträge für \(A\) als vollbesetzte Matrix beträgt \({(n -
1)}^4\), während die Anzahl der nicht-Nulleinträge durch \(5n^2 - 14n + 9\)
gegeben ist. Dieser Wert ergibt sich aus der expliziten Zählung der
nicht-Nulleinträge in der Matrix \(A\).

\pagebreak

\section{Experimentelle Untersuchungen}

Als Grundlage für unsere experimentellen Untersuchungen wurde die Funktion
\begin{equation}\label{eq:u}
    u(x) \coloneq x_1 \cdot \sin(3 \cdot x_1) \cdot x_2 \cdot \sin(3 \cdot x_2)
\end{equation}
definiert. Für diese Funktion lässt sich die folgende Beziehung herleiten:
\begin{equation}\label{eq:f}
    f(x) \coloneq -6 \paren*{\begin{array}{l}
            x_1 \cdot \cos(3 \cdot x_2) \cdot \sin(3 \cdot x_1)   \\
            + x_2 \cdot \cos(3 \cdot x_1) \cdot \sin(3 \cdot x_2) \\
            - 3 \cdot x_1 \cdot x_2 \sin(3 \cdot x_1) \cdot \sin(3 \cdot x_2)
        \end{array}}
    = -\Delta u(x).
\end{equation}
Unter Verwendung dieser Funktion \(f\) wurde eine numerische Approximation \(\hat u\)
des Poisson-Problems unter Einbeziehung der vorgegebenen Anfangsbedingungen
berechnet und mit der analytischen Lösung \(u\) verglichen. Die experimentellen
Untersuchungen wurden durchgeführt, um zwei zentrale Fragestellungen zu adressieren:
Erstens die Identifikation des optimalen Speicherformats für die Systemmatrix \(A\)
und zweitens die Analyse des Konvergenzverhaltens der numerischen Approximation
bezüglich des Poisson-Problems.

Der erste Teil der experimentellen Untersuchungen konzentrierte sich auf die
Systemmatrix \(A\) und deren Speicherformate. Zunächst wurde die Struktur der
Matrix \(A\) hinsichtlich ihrer Dünnbesetztheit (Sparsity) sowie der
Dünnbesetztheit ihrer LU-Zerlegung untersucht.

\begin{figure}[H]\label{fig:sparsity}
    \centering
    \includegraphics[width=\textwidth]{sparsity.pdf}
    \caption{ Dünnbesetztheit der Matrix \(A\) in Abhängigkeit von \(N\).}
\end{figure}

\begin{figure}[H]\label{fig:sparsity-lu}
    \centering
    \includegraphics[width=\textwidth]{sparsity-lu.pdf}
    \caption{
        Dünnbesetztheit der LU-Zerlegung der Matrix \(A\) in Abhängigkeit von
        \(N\).
    }
\end{figure}

Darüber hinaus wurde der theoretische Speicherbedarf der Matrix \(A\) im
vollbesetzten (dense) und dünnbesetzten (sparse) Format systematisch
analysiert.

\begin{figure}[H]\label{fig:theoretical-memory-usage}
    \centering
    \includegraphics[width=\textwidth]{theoretical-memory-usage.pdf}
    \caption{
        Theoretische Speicherbedarfsanalyse der Matrix \(A\) im dense und sparse
        Format in Abhängigkeit von \(N\).
    }
\end{figure}

Im zweiten Teil der experimentellen Untersuchungen wurde die numerische Lösung
\(\hat u\) mit der analytischen Lösung \(u\) grafisch verglichen. Hierzu wurden
beide Funktionen für \(n \in \set{4, 11, 128}\) visualisiert.

\begin{figure}[H]\label{fig:solutions-for-n-equal-4}
    \centering
    \includegraphics[width=\textwidth]{solutions-for-n-equal-4.pdf}
    \caption{Vergleich der analytischen und numerischen Lösungen für \(n = 4\).}
\end{figure}

\begin{figure}[H]\label{fig:solutions-for-n-equal-11}
    \centering
    \includegraphics[width=\textwidth]{solutions-for-n-equal-11.pdf}
    \caption{Vergleich der analytischen und numerischen Lösungen für \(n = 11\).}
\end{figure}

\begin{figure}[H]\label{fig:solutions-for-n-equal-128}
    \centering
    \includegraphics[width=\textwidth]{solutions-for-n-equal-128.pdf}
    \caption{Vergleich der analytischen und numerischen Lösungen für \(n = 128\).}
\end{figure}

Zusätzlich wurde der Fehler zwischen der approximierten und der exakten Lösung
systematisch untersucht. Hierzu wurde zunächst der maximale Fehler in
Abhängigkeit von \(n\) analysiert und anschließend die Fehlerverteilung an den
einzelnen Diskretisierungspunkten in 3D sowie im Heatmap-Format dargestellt.

\begin{figure}[H]\label{fig:error}
    \centering
    \includegraphics[width=\textwidth]{error.pdf}
    \caption{
        Maximaler Fehler zwischen der analytischen und numerischen Lösung in
        Abhängigkeit von \(n\).
    }
\end{figure}

\begin{figure}[H]\label{fig:difference-for-n-equal-4}
    \centering
    \includegraphics[width=\textwidth]{difference-for-n-equal-4.pdf}
    \caption{
        Fehlerverteilung zwischen der analytischen und numerischen Lösung für
        \(n = 4\).
    }
\end{figure}

\begin{figure}[H]\label{fig:difference-for-n-equal-11}
    \centering
    \includegraphics[width=\textwidth]{difference-for-n-equal-11.pdf}
    \caption{
        Fehlerverteilung zwischen der analytischen und numerischen Lösung für
        \(n = 11\).
    }
\end{figure}

\begin{figure}[H]\label{fig:difference-for-n-equal-128}
    \centering
    \includegraphics[width=\textwidth]{difference-for-n-equal-128.pdf}
    \caption{
        Fehlerverteilung zwischen der analytischen und numerischen Lösung für
        \(n = 128\).
    }
\end{figure}

\pagebreak

\subsection{Beobachtungen}

\begin{itemize}
    \item In \autoref{fig:sparsity} ist zu sehen, dass, für größer werdende
          \(N\), die Anzahl an Gesamteinträgen in \(A\) im Log-Log Plot
          proportional zu \(N^2\) verläuft, während die Anzahl an
          nicht-Nulleinträgen proportional zu \(N\) verläuft und damit
          langsamer wächst. Auch zu sehen ist, dass die relative Sparsity gegen
          0 geht und zwar proportional zu \(N^{-1}\). Auch für die
          LU-Zerlegung, zu sehen in \autoref{fig:sparsity-lu}, ist ähnliches
          Verhalten zu erkennen.

    \item In \autoref{fig:theoretical-memory-usage} ist der theoretische
          Speicherplatzbedarf in Abhängigkeit von \(N\) im Log-Log Plot
          dargestellt. Für größer werdende Werte von \(N\) steigt der
          Speicherplatzbedarf für das raw-Format stärker an, als das
          CRS-Format, also das Format um \(A\) als sparse-Matrix zu speichern.
          Es ist zu sehen, dass der Speicherplatzbedarf für das raw-Format
          proportional zu \(N^2\) und der des CRS-Format proportional zu \(N\)
          ist.

    \item In \autoref{fig:solutions-for-n-equal-4},
          \autoref{fig:solutions-for-n-equal-11}, und
          \autoref{fig:solutions-for-n-equal-128} ist der Vergleich der
          analytischen Lösung mit den approximierten Lösungen für verschiedene
          Werte von \(n\) dargestellt. Es ist zu beobachten, dass, für größer
          werdende Werte von \(n\), die approximierte Lösung sich der
          analytischen Lösung immer mehr annähert.

    \item In \autoref{fig:error} ist der maximale Fehler zwischen der
          numerischen und analytischen Lösung in Abhängigkeit von \(N\) im
          Log-Log Plot zu sehen. Man kann beobachten, dass der Fehler zuerst
          fällt und wieder hoch geht, aber dann ab ca. \(10^{1}\) immer kleiner
          wird. Ab diesem Wert verläuft die Kurve proportional zu \(N^{-2}\).

    \item In \autoref{fig:difference-for-n-equal-4},
          \autoref{fig:difference-for-n-equal-11}, und
          \autoref{fig:difference-for-n-equal-128} ist der Fehler an den
          einzelnen Diskretisierungspunkten, im 3D und Heatmap Plot, zu sehen.
          Für \(n = 4\) liegt der Fehler zwischen ungefähr 0.04 und 0.14, für
          \(n = 11\) zwischen ungefähr 0.005 und 0.035, und für \(n = 128\)
          zwischen ungefähr 0.00005 und 0.00030. Es ist also eine Verringerung
          des Fehlers an den einzelnen Diskretisierungspunkten für größer
          werdende \(n\) zu beobachten.
\end{itemize}

\section{Auswertung}

\begin{itemize}
    \item Der stärkere Anstieg in \autoref{fig:sparsity} der Matrix \(A\) im
          vollbesetzten Format verglichen mit dem dünnbesetzten Format
          entspricht den Erwartungen, denn die Anzahl der Gesamteinträge ist
          wie in der Theorie erwähnt gleich \({(n - 1)}^4 = N^2\) was
          proportional zu \(n^4\) bzw.\ \(N^2\) ist, und die Anzahl der
          nicht-Null Einträge ist gleich \(5n^2 - 14n + 9\) was proportional zu
          \(n^2\) bzw.\ proportional zu \(N\) ist. Auch dass die relative
          Sparsity im Graphen proportional zu \(n^{-2}\), also \(N^{-1}\)
          verläuft, ist hiermit zu erklären, da nach Definition die relative
          Sparsity gleich \(\frac{5n^2 - 14n + 9}{{(n - 1)}^4}\) ist, was
          offensichtlich proportional zu \(n^{-2}\) ist, also in
          \(\BigO(n^{-2}) = \BigO(N^{-1})\) liegt.

    \item Auch der stärkere Anstieg des theoretischen Speicherbedarfs in
          \autoref{fig:theoretical-memory-usage} ist wie erwartet, da für das
          raw-Format jeder Eintrag einmal abgespeichert werden muss, also liegt
          hier das Speichern in \(\BigO(n^4) = \BigO(N^2)\) und für das
          CRS-Format für jeden nicht-Null-Eintrag drei Werte gespeichert werden
          müssen, was proportional zu \(n^2\) ist, also in \(\BigO(n^2) =
          \BigO(N)\) liegt.

    \item Der grafische Vergleich in \autoref{fig:solutions-for-n-equal-4},
          \autoref{fig:solutions-for-n-equal-11}, und
          \autoref{fig:solutions-for-n-equal-128} zeigt, dass die numerische
          Lösung für größere Werte von \(n\) immer besser mit der analytischen
          Lösung übereinstimmt.

    \item Dies wird ferner von \autoref{fig:difference-for-n-equal-4},
          \autoref{fig:difference-for-n-equal-11}, und
          \autoref{fig:difference-for-n-equal-128} bestätigt, denn der maximale
          Fehler zwischen der numerischen und analytischen Lösung verläuft für
          \(N \geqslant 10\) proportional zu \(N^{-2}\) und fällt damit ab.

    \item Auch \autoref{fig:difference-for-n-equal-4},
          \autoref{fig:difference-for-n-equal-11}, und
          \autoref{fig:difference-for-n-equal-128} zeigt dies nochmal durch
          einen Plot des Fehlers an den verschiedenen Diskretisierungspunkten.
          Während der Fehler zwar variiert, so wird er für größere Werte von
          \(n\) immer kleiner und liegt beispielsweise für \(n = 128\) nur noch
          ungefähr 0.00005 und 0.00030.

    \item Dieses Verhalten stimmt auch mit der Theorie überein, denn die
          analytische Lösung ist, für das gesamte Gebiet \(\Omega\) definiert,
          und da je größer man \(n\) wählt, desto größer wird auch \(N\) und
          deckt somit mehr Punkte des Gebietes ab und die numerische Lösung
          konvergiert daher gegen die analytische Lösung.
\end{itemize}

\section{Zusammenfassung}

Untersucht wurde das numerische Approximieren einer Lösung des Poisson-Problems
für das Gebiet \(\Omega = {(0, 1)}^2\) und \(g = 0\). Dafür haben wir das
Gebiet in ein Gitter von \({(n - 1)}^2 = N\) Diskretisierungs-Punkten
aufgeteilt und die zu suchende Funktion an diesen Stellen mithilfe eines
Gleichungssystems approximiert. Zuerst wurde die Matrix \(A\), die das
Gleichungssystem beschreibt untersucht. Unsere Ergebnisse sind, dass die Anzahl
der Gesamteinträge viel schneller steigt, als die der nicht-Null-Einträge, was
ein Speichern dieser Matrix im CRS-Format sinnvoll macht. Selbiges Ergebnis
fanden wir auch für die LU-Zerlegung von \(A\), die zum Lösen des
Gleichungssystems diente, dieser Matrix. Zum Untersuchen des
Konvergenzverhaltens der Approximation von \autoref{eq:u} nahmen wir uns die
Funktion in \autoref{eq:f} welche die analytische Lösung für das
Poisson-Problem ist. Wir haben unsere numerische Lösung für verschiedene Werte
von \(n\) mit der analytischen Lösung verglichen und fanden, dass der maximale
Fehler zwischen analytischer und numerischer Lösung in \(\BigO({(n - 1)}^{-4})
= BigO(N^{-2})\) liegt. Auch grafisch ist zu sehen, dass die numerische Lösung
für größere \(n\) der Funktion \(u\) immer mehr ähnelt bzw.\ gar nicht mehr zu
unterscheiden ist.

\printbibliography%

\end{document}
