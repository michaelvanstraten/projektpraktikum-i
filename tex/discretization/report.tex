\documentclass{scrartcl}

\usepackage{csquotes}
% Babel provides hyphenation patterns and translations of keywords like 'table
% of contents'
\usepackage[ngerman]{babel}
\usepackage{biblatex}
% Automatic generation of hyperlinks for references and URIs
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{luatodonotes}
% Provides additional symbols
% Provides environments for typical math text structure


\include{common.tex}

\KOMAoptions{
  % Add vertical space between two paragraphs, without indent
  parskip=true,
}

\subject{Bericht}
\titlehead{%
  \begin{minipage}{.7\textwidth}%
  Humboldt-Universit\"at zu Berlin\\
  Mathematisch-Naturwissenschaftliche Fakult\"at\\
  Institut f\"ur Mathematik
  \end{minipage}
}
\title{Lösen des Poisson-Problems mittels SOR-Verfahren}
\author{%
  Eingereicht von M. van Straten und P. Merz
}
\date{\today}

\graphicspath{{./figures/}}

\addbibresource{report.bib}

\begin{document}


\maketitle
\tableofcontents
\cleardoublepage%

%TODO ADD CITATIONS
\section{Einleitung und Motivation}
Im Laufe des Moduls "Projekt Praktikum I" haben wir uns zuerst mit dem Thema
Finite Differenzen beschäftigt. Mit hilfe dieser haben wir das Poisson-Problem
\begin{align}\label{Eq:Poisson}
    -\Delta u & = f \; \text{in} \; \Omega           \\
    u         & = g \; \text{auf} \; \partial \Omega
\end{align}
wobei
\begin{equation*}
    \Delta u = \frac{\partial^2 u}{\partial x_1^2} + \frac{\partial^2 u}{\partial x_2^2}
\end{equation*}
den Laplace-Operator von \(u\) darstellt, für eine gegebene Funktion \(f\)
auf \(\Omega = {(0, 1)}^2\) und \(g = 0\) diskretisiert und anschließend mit
der LU-Zerlegung der entstehenden Diskretisierungsmatrix \(A\) gelöst. Während die Matrix \(A\) zwar dünn besetzt war,
war die LU-Zerlegung dieser nicht dünn besetzt. Aufgrund dessen wollen wir in diesem Bericht eine weitere Art des
Lösens für das entstehende Gleichungssystem betrachten: Das SOR-Verfahren. Dieses Verfahren ist ein iteratives Verfahren, welches wir in diesem hinsichtlich der Konvergenzrate untersuchen und ferner mit dem LU-Verfahren vergleichen wollen.

\section{Theoretische Grundlagen}

\subsection{Poisson-Problem (diskretisiert)}
Durch Diskretisieren des Laplace-Operators und des Gebietes auf \((n-1)^2\)
äquidistanten Gitterpunkten mit Feinheit \(h=\frac{1}{n}\) wird Gleichung \ref{Eq:Poisson} zu einem Gleichungssystem in \((n-1)^2\) Unbekannten. Für eine genaue Herleitung des Gleichungssystems sowie die Struktur der Koeffezientenmatrix siehe \cite{HandoutLU}.

\subsection{Iterative Verfahren}
Sei im folgenden immer ein lineares Gleichungssystem
\[Ax=b\]
mit \(A \in \R^{n \times n}\) regulär und \(b \in \R^n\) gegeben.
\begin{definition}\cite[p.~69-70]{Iterative}
    Ein Iterationsverfahren ist gegeben durch die Abbildung
    \[\phi:\R^n \times \R^n \rightarrow \R^n\]
    mit Iterationsvorschrift
    \[x^{(k+1)}=\phi(x^{(k)},b).\]
    Das Iterationsverfahren heißt linear, falls \(B,C \in \R^{n \times n}\)
    existieren, sodass
    \[\phi(x,b)=Bx+Cb.\]
    Das Iterationsverfahren heißt konsistent zur Matrix \(A\), falls die Lösung des linearen Gleichungssystems ein Fixpunkt des Iterationsverfahrens ist.
    Das Iterationsverfahren heißt konvergent, falls für alle \(x^{(0)} \in \R^n\) der Grenzwert
    \[\tilde{x} = \lim_{k \to \infty} \phi(x^{(k)},b)\] existiert.
\end{definition}
Für mehr Informationen zur Konsistenz und Konvergenz, siehe \cite[p.~71-72]{Iterative}.



\subsection{Splitting-Verfahren}
Splitting-Verfahren basieren auf dem Zerlegen der Koeffizientenmatrix \(A\) in
zwei Matrizen \(M, N \in \R^{n \times n}\), mit \(M\) invertierbar, sodass
\[A = M-N\]
gilt. Für das lineare Gleichungssystem gilt dann
\begin{align*}
    \iff Ax     & = b                  \\
    \iff (M-N)x & = b                  \\
    \iff Mx     & = Nx + b             \\
    \iff x      & = M^{-1}Nx + M^{-1}b
\end{align*}
Das dazugehörige Iterationsverfahren lautet\cite[p.~165]{SOR}
\[x^{(k+1)}= M^{-1}Nx^{(k)} + M^{-1}b.\]
Falls für den Spektralradius von \(M^{-1}N\) zusätzlich noch
\[\rho(M^{-1}N)<1\]
gilt, so konvergiert das Verfahren gegen die Lösung des linearen Gleichungssystems\cite[p.~73-74]{Iterative}.

\subsection{SOR-Verfahren}
Das SOR-Verfahren (Successive Over-Relaxation-Verfahren) ist ein solches Splitting-Verfahren. Um dieses herzuleiten, teilt man die Koeffizientenmatrix in eine Diagonalmatrix \(D\), eine strikte untere Matrix \(L\) und eine strikte obere Matrix \(U\) auf, sodass \(A=D-L-U\) mit 
\[
    D = \begin{pmatrix}
        a_{11} & 0      & \cdots & 0      \\
        0      & a_{22} & \cdots & 0      \\
        \vdots & \vdots & \ddots & \vdots \\
        0      & 0      & \cdots & a_{nn}
    \end{pmatrix}, \quad
    L = \begin{pmatrix}
        0       & 0       & \cdots & 0      \\
        -a_{21} & 0       & \cdots & 0      \\
        \vdots  & \vdots  & \ddots & \vdots \\
        -a_{n1} & -a_{n2} & \cdots & 0
    \end{pmatrix}, \quad
    R = \begin{pmatrix}
        0      & -a_{12} & \cdots & -a_{1n} \\
        0      & 0       & \cdots & -a_{2n} \\
        \vdots & \vdots  & \ddots & \vdots  \\
        0      & 0       & \cdots & 0
    \end{pmatrix}
.\]
Wählt man nun einen Relaxationsfaktor \(\omega \neq 0\), so lautet die
die Iterationsvorschrift für das SOR-Verfahren zur Matrix \(A\)
\[x^{(k+1)}=(D - \omega L)^{-1}[(1- \omega)D + \omega U]x^{(k)} + \omega (D-\omega U)^{-1}b\]

Nutzt man die untere Dreiecksstruktur von \(D - \omega L\) aus, kann man
mittels Vorwärtssubstitution die einzelnen Einträge von \(x^{(k+1)}\) berechnen
und es gilt:
\[
    x_i^{(k+1)} = (1 - \omega) x_i^{(k)} + \frac{\omega}{a_{ii}}
    \left( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right),
    \quad i = 1, 2, \ldots, n.
\]
Für eine genaue Herleitung des SOR-Verfahrens siehe \cite[p.~179-180]{SOR}.

\subsection{Konvergenz des SOR-Verfahren und optimaler Relaxationsfaktor}
\begin{theorem}
    Sei \(A \in \mathbb{R}^n\) symmetrisch und positiv definit, dann gilt aufgrund der Symmetrie
    für die Zerlegung von \(A\), dass
    \[U = L^T.\]
    Daraus folgt: Das SOR-Verfahren konvergiert nach \cite[p.~183-185]{SOR} für \(0 < \omega < 2\) für alle
    Startwerte \(x^{(0)} \in \mathbb{R}^n\). \\
    Außerdem existiert, wie in \cite[p.~4]{omega_opt} beschrieben, ein optimaler Relaxationsfaktor \(\omega_{opt}\), der die Konvergenzrate optimiert mit
    \[\omega_{opt}=\frac{2}{1+\sqrt{1-\beta^2}}\]
    wobei \(\beta=\rho(I-D^{-1}A)\).
\end{theorem}


\subsubsection{Anwendung auf die Matrix \(A_{p}\)}
Die Matrix \(A_p\), die sich durch Diskretisierung des Laplace-Operators ergibt, ist aufgrund ihrer Struktur trivialerweise symmetrisch. Ferner ist sie auch positiv definit\cite[p.~11]{PosDef}.
Also konvergiert das SOR-Verfahren für \(\omega \in (0, 2)\) für diese Matrix.
\\
Ferner lässt sich der optimale Relaxationsfaktor, in Abhängigkeit der Feinheit \(h\) unserer Diskretisierung, für diese Matrix herleiten und nach \cite{omega_opt} gilt
\[\omega_{opt}=\frac{2}{1+\sin(\pi h)}.\] 


\subsection{Computerarithmetik und Kondition einer Matrix}
Wie in \cite[p.~3]{HandoutFiniteDifferenzen} beschrieben, werden Zahlen auf einem Computer mithilfe von Bits dargestellt, und deshalb ist die Menge
an darstellbaren Zahlen beschränkt, weshalb es zu Ungenauigkeiten bei Rechenoperationen kommen kann.
Dies kann dazu führen, dass unser exaktes Gleichungssystem auf dem Computer zu einem gestörten
Gleichungssystem wird.
Dazu zuerst folgende Definition

\begin{definition}\cite[p.~25]{Iterative}
    Sei \(A \in \R^n\) invertierbar und \(\| \phantom{x}\|\) eine Matrixnorm, dann bezeichnet
    \[\operatorname{cond}(A)=\|A\| \cdot\|A^{-1}\|\]
    die Kondition von \(A\) bezüglich der gegebenen Matrixnorm.
\end{definition}
Die Kondition einer Matrix ist sehr wichtig, für das Lösen von Gleichungssystemen, denn sie wirkt sich wie folgt aus
\begin{theorem}\cite[p.~26-27]{Iterative}
    Es sei ein Gleichungssystem \(Ax=b\) mit \(A\) invertierbar gegeben, \(e_k = A^{-1}b-x_k\) der Fehlervektor und \(r_k = b - Ax_k\) der Residuenvektor der \(k\)-ten Iteration beim Lösen eines Gleichungssystem mithilfe eines Iterationsverfahrens.
    Dann gilt
    \[\frac{1}{\operatorname{cond}(A)} \frac{\|r_k\|}{\|r_0\|} \leq \frac{\|e_k\|}{\|e_0\|} \leq \operatorname{cond}(A) \frac{\|r_k\|}{\|r_0\|} \leq \operatorname{cond}(A)^2 \frac{\|e_k\|}{\|e_0\|}.\]
    \\
    Sei nun \(x+\Delta x\) die Lösung des gestörten Gleichungssystems \(A(x + \Delta x)= b+ \Delta b\), dann gilt
    \[\frac{\|\Delta x\|}{\|x\|} \leq \operatorname{cond}(A) \frac{\|\Delta b\|}{\|b\|}.\]
\end{theorem}
Das heißt wenn unsere Matrix schlecht konditioniert ist, also die Kondition von \(A\) sehr groß ist, dass auch mit kleiner werdendem Residuum, eine deutlich große Fehlernorm vorliegen kann.

\subsection{Sparsity}
Eine Matrix \(A \in \R^n \), heißt dünnbesetzt, falls sie zum Großteil aus Nulleinträgen besteht.
Eine Konsequenz dessen ist, dass man für das Speichern solcher Matrizen auf dem Computer nach effizienten Methoden sucht, oder geeigneten Algorithmen für Probleme, in denen dünnbesetzte Matrizen auftauchen, entwickelt. Dabei wird das Verhältnis von Anzahl der Nulleinträge zu Anzahl der Gesamteinträge Sparsity genannt. Solche Matrizen kommen beispielsweise beim Diskretisieren von partiellen Differentialgleichungen in der Numerik hervor \cite{dewiki:233309191}.
Ein konkretes Beispiel wäre die Diskretisierung des Poisson-Problems in zwei Dimensionen mit gegeben Randbedingungen, siehe \cite{HandoutLU}.





\section{Experimentelle Untersuchungen}

Für die folgenden Experimente definieren wir die Funktion \definitionU sowie
\definitionF mit \(\kappa = 3\), für welche gelten das
\[
    - \Delta u(x) = f(x)
\]
ist.

\subsection{Lösung des Poisson-Problems}

Ähnlich wie zur Lösung der Poisson-Gleichung via LU-Zerlegung wollen wir
untersuchen, ob und wie sich der maximalen absoluten Fehler entwickelt.
Hierfür wollen wir diesen Grafisch darstellen, was wir mit dem folgenden code
Snippet erreicht haben.

% [TODO] Add code snippet

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{error_plot_eps1}
    \caption{Maximaler absoluter Fehler in Abhängigkeit der Iterationen für
        $\epsilon = 10^{-6}$}
    \label{fig:error_plot_eps1}
\end{figure}

Wir sehen das der maximale Fehler für größere \(n\) schneller gegen 0 zu
konvergieren scheint dann allerdings für \(n = \todo{Wert ergänzen}\) wieder
ansteigt. Dies könnte an der Wahl des Abbruchkriteriums liegen, welches wir auf
\(\epsilon = 10^{\todo{Wert ergänzen}}\) gesetzt haben. Für größere \(n\)
könnte es sein das dieses Kriterium zu streng ist und das Verfahren deshalb
abbricht bevor es konvergiert.

Um dies zu überprüfen haben wir dieselbe Untersuchung für \(\epsilon =
10^{\todo{Wert ergänzen}}\)erneut durchgeführt.

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{error_plot_eps2}
    \caption{Maximaler absoluter Fehler in Abhängigkeit der Iterationen für
        $\epsilon = 10^{-8}$}
    \label{fig:error_plot_eps2}
\end{figure}

\subsection{Optimale Wahl von \(\epsilon\)}\label{sec:optimal_epsilon}

Wie wir in den vorherigen Experimenten gesehen haben, hat die Wahl von
\(\epsilon\) einen großen Einfluss auf die Konvergenz des SOR-Verfahrens.
Deshalb wollen wir nun untersuchen wie sich der maximale absolute Fehler in
Abhängigkeit von \(\epsilon\) entwickelt.

Hierfür haben wir uns den maximalen absoluten Fehler für
\[
    \epsilon = h^k \quad \text{mit} \quad
    k \in \set{-2, 0, 2, 4, 6}
\]
angesehen.

Der code Snippet um dies zu erreichen sieht wie folgt aus:

% [TODO] Add code snippet

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{optimal_epsilon}
    \caption{Maximaler absoluter Fehler für verschiedene \(\epsilon\)}
    \label{fig:optimal_epsilon}
\end{figure}

Wir sehen das mit steigendem \(k\) der maximale Fehler abnimmt. Für \(k = 4\)
und \(k = 6\) scheint der Graph allerdings nahezu identisch zu sein.

Dies ist auch zu erwarten, da für kleinere \(\epsilon\) das Verfahren zu früh
abbricht und für größere \(\epsilon\) das Verfahren zu lange läuft. Der Fakt,
dass der Graph für \(\epsilon = h^{-6}\) gleich zu dem für \(\epsilon =
h^{-4}\) ist, lässt folgern das \(\epsilon = h^{-4}\) die optimale Wahl ist da
weitere Iterationen keinen zusätzlichen Nutzen bringen.

\subsection{Optimaler Relaxationsparameter \(\omega\)}

% [TODO]

\subsection{Vergleich mit dem LU-Verfahren}

Da wir bereits das LU-Verfahren zu Lösung des Poisson-Problems untersucht haben
scheint es sinnvoll die beiden Verfahren miteinander zu vergleichen.

\subsubsection{Konvergenz verhalten}

Hierfür betrachten wir zunächst inwiefern beide Verfahren in der Lage sind, die
exakte Lösung zu approximieren. Hierfür haben wir wiederum den maximalen
absoluten Fehler in Abhängigkeit der Iterationen betrachtet. Für das
SOR-Verfahren haben wir \(\epsilon = 10^{-8}\) fest sowie optimal wie in
\autoref{sec:optimal_epsilon} bestimmt.

Um den folgenden Graphen zu erstellen haben wir den folgenden code Snippet
verwendet:

% [TODO] Add code snippet

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{convergence-comparison}
    \caption{Vergleich des maximalen absoluten Fehlers für das LU- und
        SOR-Verfahren}
    \label{fig:convergence-comparison}
\end{figure}

Wir beobachten das beide Verfahren bis \(n = \todo{Wert ergänzen}\) sehr
ähnlich konvergieren. Ab diesem Punkt steigt der maximale Fehler für das
SOR-Verfahren mit \(\epsilon = 10^{-8}\) wieder an. Ab \(n = \todo{Wert
ergänzen}\) geschieht für das SOR-Verfahren mit optimalen \(\epsilon\)
dasselbe.

\subsubsection{Laufzeitverhalten}

Ein wesentlicher Faktor bei der Wahl des Verfahrens ist dessen Laufzeit. Um
dies zu untersuchen haben wir die Laufzeit beider Verfahren in Abhängigkeit der
Diskretisierung Feinheit \(n\) betrachtet. Das SOR-Verfahren haben wir dabei
mit optimalen \(\epsilon\) optimalem \(\omega\) betrachtet. Die dargestellten
Laufzeiten sind dabei gemittelt über \(\todo{Wert ergänzen}\) Durchläufe, um
Beeinflussungen durch andere Prozesse zu minimieren.

Der code Snippet um dies zu erreichen sieht wie folgt aus:

% [TODO] Add code snippet

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{runtime-comparison}
    \caption{Vergleich der Laufzeiten des LU- und SOR-Verfahrens}
    \label{fig:runtime-comparison}
\end{figure}

\section{Auswertung}

% [TODO]

\section{Zusammenfassung}

% [TODO]

\printbibliography

\end{document}