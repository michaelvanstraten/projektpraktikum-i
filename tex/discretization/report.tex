\documentclass{scrartcl}

\usepackage{csquotes}
% Babel provides hyphenation patterns and translations of keywords like 'table
% of contents'
\usepackage[ngerman]{babel}
\usepackage{biblatex}
% Automatic generation of hyperlinks for references and URIs
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{luatodonotes}
\usepackage[newfloat]{minted}
\usepackage{caption}

\include{common.tex}

\newenvironment{code}{\captionsetup{type=listing}}{}
\SetupFloatingEnvironment{listing}{name=Source Code}

\KOMAoptions{
  % Add vertical space between two paragraphs, without indent
  parskip=true,
}

\subject{Bericht}
\titlehead{%
  \begin{minipage}{.7\textwidth}%
  Humboldt-Universität zu Berlin\\
  Mathematisch-Naturwissenschaftliche Fakultät\\
  Institut für Mathematik
  \end{minipage}
}
\title{Lösen des Poisson-Problems mittels SOR-Verfahren}
\author{%
  Eingereicht von M. van Straten und P. Merz
}
\date{\today}

\graphicspath{{./figures/}}

\addbibresource{report.bib}

\begin{document}

\maketitle
\tableofcontents
\cleardoublepage%

% [TODO] Add citations

\section{Einleitung und Motivation}

Im Laufe des Moduls ``Projekt Praktikum I'' haben wir uns zuerst mit dem Thema
Finite Differenzen beschäftigt. Mit hilfe dieser haben wir das Poisson-Problem
\begin{align}\label{Eq:Poisson}
    -\Delta u & = f \; \text{in} \; \Omega           \\
    u         & = g \; \text{auf} \; \partial \Omega
\end{align}
wobei
\begin{equation*}
    \Delta u = \frac{\partial^2 u}{\partial x_1^2} + \frac{\partial^2 u}{\partial x_2^2}
\end{equation*}
den Laplace-Operator von \(u\) darstellt, für eine gegebene Funktion \(f\)
auf \(\Omega = {(0, 1)}^2\) und \(g = 0\) diskretisiert und anschließend mit
der LU-Zerlegung der entstehenden Diskretisierungsmatrix \(A\) gelöst. Während die Matrix \(A\) zwar dünn besetzt war,
war die LU-Zerlegung dieser nicht dünn besetzt. Aufgrund dessen wollen wir in diesem Bericht eine weitere Art des
Lösens für das entstehende Gleichungssystem betrachten: Das SOR-Verfahren. Dieses Verfahren ist ein iteratives Verfahren, welches wir in diesem hinsichtlich der Konvergenzrate untersuchen und ferner mit dem LU-Verfahren vergleichen wollen.

\section{Theoretische Grundlagen}

\subsection{Poisson-Problem (diskretisiert)}

Durch Diskretisieren des Laplace-Operators und des Gebietes auf \((n-1)^2\)
äquidistanten Gitterpunkten mit Feinheit \(h=\frac{1}{n}\) wird Gleichung
\ref{Eq:Poisson} zu einem Gleichungssystem in \((n-1)^2\) Unbekannten. Für eine
genaue Herleitung des Gleichungssystems sowie die Struktur der
Koeffezientenmatrix siehe \cite{HandoutLU}.

\subsection{Iterative Verfahren}

Sei im folgenden immer ein lineares Gleichungssystem
\[Ax=b\]
mit \(A \in \R^{n \times n}\) regulär und \(b \in \R^n\) gegeben.
\begin{definition}\cite[p.~69-70]{Iterative}
    Ein Iterationsverfahren ist gegeben durch die Abbildung
    \[\phi:\R^n \times \R^n \rightarrow \R^n\]
    mit Iterationsvorschrift
    \[x^{(k+1)}=\phi(x^{(k)},b).\]
    Das Iterationsverfahren heißt linear, falls \(B,C \in \R^{n \times n}\)
    existieren, sodass
    \[\phi(x,b)=Bx+Cb.\]
    Das Iterationsverfahren heißt konsistent zur Matrix \(A\), falls die Lösung
    des linearen Gleichungssystems ein Fixpunkt des Iterationsverfahrens ist.
    Das Iterationsverfahren heißt konvergent, falls für alle \(x^{(0)} \in
    \R^n\) der Grenzwert
    \[\tilde{x} = \lim_{k \to \infty} \phi(x^{(k)},b)\] existiert.
\end{definition}
Für mehr Informationen zur Konsistenz und Konvergenz, siehe \cite[p.~71-72]{Iterative}.

\subsection{Splitting-Verfahren}

Splitting-Verfahren basieren auf dem Zerlegen der Koeffizientenmatrix \(A\) in
zwei Matrizen \(M, N \in \R^{n \times n}\), mit \(M\) invertierbar, sodass
\[A = M-N\]
gilt. Für das lineare Gleichungssystem gilt dann
\begin{align*}
    \iff Ax     & = b                  \\
    \iff (M-N)x & = b                  \\
    \iff Mx     & = Nx + b             \\
    \iff x      & = M^{-1}Nx + M^{-1}b
\end{align*}
Das dazugehörige Iterationsverfahren lautet\cite[p.~165]{SOR}
\[x^{(k+1)}= M^{-1}Nx^{(k)} + M^{-1}b.\]
Falls für den Spektralradius von \(M^{-1}N\) zusätzlich noch
\[\rho(M^{-1}N)<1\]
gilt, so konvergiert das Verfahren gegen die Lösung des linearen
  Gleichungssystems\cite[p.~73-74]{Iterative}.

\subsection{SOR-Verfahren}
Das SOR-Verfahren (Successive Over-Relaxation-Verfahren) ist ein solches
Splitting-Verfahren. Um dieses herzuleiten, teilt man die Koeffizientenmatrix
in eine Diagonalmatrix \(D\), eine strikte untere Matrix \(L\) und eine strikte
obere Matrix \(U\) auf, sodass \(A=D-L-U\) mit
\[
    D = \begin{pmatrix}
        a_{11} & 0      & \cdots & 0      \\
        0      & a_{22} & \cdots & 0      \\
        \vdots & \vdots & \ddots & \vdots \\
        0      & 0      & \cdots & a_{nn}
    \end{pmatrix}, \quad
    L = \begin{pmatrix}
        0       & 0       & \cdots & 0      \\
        -a_{21} & 0       & \cdots & 0      \\
        \vdots  & \vdots  & \ddots & \vdots \\
        -a_{n1} & -a_{n2} & \cdots & 0
    \end{pmatrix}, \quad
    R = \begin{pmatrix}
        0      & -a_{12} & \cdots & -a_{1n} \\
        0      & 0       & \cdots & -a_{2n} \\
        \vdots & \vdots  & \ddots & \vdots  \\
        0      & 0       & \cdots & 0
    \end{pmatrix}
    .\]
Wählt man nun einen Relaxationsfaktor \(\omega \neq 0\), so lautet die die
  Iterationsvorschrift für das SOR-Verfahren zur Matrix \(A\)
\[x^{(k+1)}=(D - \omega L)^{-1}[(1- \omega)D + \omega U]x^{(k)} + \omega (D-\omega U)^{-1}b\]

Nutzt man die untere Dreiecksstruktur von \(D - \omega L\) aus, kann man
mittels Vorwärtssubstitution die einzelnen Einträge von \(x^{(k+1)}\) berechnen
und es gilt:
\[
    x_i^{(k+1)} = (1 - \omega) x_i^{(k)} + \frac{\omega}{a_{ii}}
    \left( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right),
    \quad i = 1, 2, \ldots, n.
\]
Für eine genaue Herleitung des SOR-Verfahrens siehe \cite[p.~179-180]{SOR}.

\subsection{Konvergenz des SOR-Verfahren und optimaler Relaxationsfaktor}
\begin{theorem}
    Sei \(A \in \mathbb{R}^n\) symmetrisch und positiv definit, dann gilt aufgrund der Symmetrie
    für die Zerlegung von \(A\), dass
    \[U = L^T.\]
    Daraus folgt: Das SOR-Verfahren konvergiert nach \cite[p.~183-185]{SOR} für \(0 < \omega < 2\) für alle
    Startwerte \(x^{(0)} \in \mathbb{R}^n\). \\
    Außerdem existiert, wie in \cite[p.~4]{omega_opt} beschrieben, ein optimaler Relaxationsfaktor \(\omega_{opt}\), der die Konvergenzrate optimiert mit
    \[\omega_{opt}=\frac{2}{1+\sqrt{1-\beta^2}}\]
    wobei \(\beta=\rho(I-D^{-1}A)\).
\end{theorem}

\subsubsection{Anwendung auf die Matrix \(A_{p}\)}
Die Matrix \(A_p\), die sich durch Diskretisierung des Laplace-Operators ergibt, ist aufgrund ihrer Struktur trivialerweise symmetrisch. Ferner ist sie auch positiv definit\cite[p.~11]{PosDef}.
Also konvergiert das SOR-Verfahren für \(\omega \in (0, 2)\) für diese Matrix.
\\
Ferner lässt sich der optimale Relaxationsfaktor, in Abhängigkeit der Feinheit \(h\) unserer Diskretisierung, für diese Matrix herleiten und nach \cite{omega_opt} gilt
\[\omega_{opt}=\frac{2}{1+\sin(\pi h)}.\] 


\subsection{Computerarithmetik und Kondition einer Matrix}
Wie in \cite[p.~3]{HandoutFiniteDifferenzen} beschrieben, werden Zahlen auf einem Computer mithilfe von Bits dargestellt, und deshalb ist die Menge
an darstellbaren Zahlen beschränkt, weshalb es zu Ungenauigkeiten bei Rechenoperationen kommen kann.
Dies kann dazu führen, dass unser exaktes Gleichungssystem auf dem Computer zu einem gestörten
Gleichungssystem wird.
Dazu zuerst folgende Definition

\begin{definition}\cite[p.~25]{Iterative}
    Sei \(A \in \R^n\) invertierbar und \(\| \phantom{x}\|\) eine Matrixnorm, dann bezeichnet
    \[\operatorname{cond}(A)=\|A\| \cdot\|A^{-1}\|\]
    die Kondition von \(A\) bezüglich der gegebenen Matrixnorm.
\end{definition}
Die Kondition einer Matrix ist sehr wichtig, für das Lösen von Gleichungssystemen, denn sie wirkt sich wie folgt aus
\begin{theorem}\cite[p.~26-27]{Iterative}
    Es sei ein Gleichungssystem \(Ax=b\) mit \(A\) invertierbar gegeben, \(e_k = A^{-1}b-x_k\) der Fehlervektor und \(r_k = b - Ax_k\) der Residuenvektor der \(k\)-ten Iteration beim Lösen eines Gleichungssystem mithilfe eines Iterationsverfahrens.
    Dann gilt
    \[\frac{1}{\operatorname{cond}(A)} \frac{\|r_k\|}{\|r_0\|} \leq \frac{\|e_k\|}{\|e_0\|} \leq \operatorname{cond}(A) \frac{\|r_k\|}{\|r_0\|} \leq \operatorname{cond}(A)^2 \frac{\|e_k\|}{\|e_0\|}.\]
    \\
    Sei nun \(x+\Delta x\) die Lösung des gestörten Gleichungssystems \(A(x + \Delta x)= b+ \Delta b\), dann gilt
    \[\frac{\|\Delta x\|}{\|x\|} \leq \operatorname{cond}(A) \frac{\|\Delta b\|}{\|b\|}.\]
\end{theorem}
Das heißt wenn unsere Matrix schlecht konditioniert ist, also die Kondition von \(A\) sehr groß ist, dass auch mit kleiner werdendem Residuum, eine deutlich große Fehlernorm vorliegen kann.

\subsection{Sparsity}
Eine Matrix \(A \in \R^n \), heißt dünnbesetzt, falls sie zum Großteil aus
Nulleinträgen besteht. Eine Konsequenz dessen ist, dass man für das Speichern
solcher Matrizen auf dem Computer nach effizienten Methoden sucht, oder
geeigneten Algorithmen für Probleme, in denen dünnbesetzte Matrizen auftauchen,
entwickelt. Dabei wird das Verhältnis von Anzahl der Nulleinträge zu Anzahl der
Gesamteinträge Sparsity genannt. Solche Matrizen kommen beispielsweise beim
Diskretisieren von partiellen Differentialgleichungen in der Numerik hervor
\cite{dewiki:233309191}. Ein konkretes Beispiel wäre die Diskretisierung des
Poisson-Problems in zwei Dimensionen mit gegeben Randbedingungen, siehe
\cite{HandoutLU}.

\section{Experimentelle Untersuchungen}

Für die folgenden Experimente definieren wir die Funktion \definitionU sowie
\definitionF mit \(\kappa = 3\), für welche gelten das
\[
    - \Delta u(x) = f(x)
\]
ist.

\subsection{Lösung des Poisson-Problems}

In diesem Abschnitt untersuchen wir die Entwicklung des maximalen absoluten
Fehlers beim Anwenden des SOR-Verfahrens auf das Poisson-Problem. Dabei
variieren wir sowohl die Gitterfeinheit \(n\) als auch das Abbruchkriterium
\(\epsilon\). Im Folgenden werden zunächst die Ergebnisse für \(\epsilon =
10^{-6}\) und anschließend für \(\epsilon = 10^{-10}\) dargestellt und
diskutiert.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{error_plot}
    \caption{Maximaler absoluter Fehler in Abhängigkeit der Iterationen für
        $\epsilon = 10^{-6}$}
    \label{fig:error_plot}
\end{figure}

\begin{code}
    \captionof{listing}{Code Snippet zum Generieren von \autoref{fig:error_plot}}
    \label{code:error_plot}
    \inputminted[firstline=3, lastline=9]{bash}{figures/generate.sh}
\end{code}

\textbf{Beobachtung:}

Für \(\epsilon = 10^{-6}\) nimmt der maximale Fehler bei kleineren und
mittleren Werten von \(n\) stetig ab, während für \(n \approx 6000\) ein
erneutes Ansteigen sichtbar wird. Dies legt den Verdacht nahe, dass bei größer
werdenden Systemen das Abbruchkriterium zu früh erfüllt ist, sodass das
SOR-Verfahren vorzeitig beendet wird.

Um diesen Effekt zu überprüfen, betrachten wir im nächsten Schritt \(\epsilon =
10^{-10}\).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{error_plot_better_eps}
    \caption{Maximaler absoluter Fehler in Abhängigkeit der Iterationen für
        $\epsilon = 10^{-10}$}
    \label{fig:error_plot_better_eps}
\end{figure}

\textbf{Beobachtung:}

Wie erwartet sinkt der maximale Fehler für \(\epsilon = 10^{-10}\) auch bei
großen Diskretisierungen weiter. Zusätzlich fällt auf, dass für \(n = 4\) ein
im Vergleich zu benachbarten Werten von \(n\) deutlich geringerer Fehler
auftritt. Ein möglicher Grund könnte sein, dass die zugrunde liegende Funktion
in diesem Bereich nahezu linear verläuft und so trotz weniger
Diskretisierungspunkte eine relativ genaue Lösung erzielt werden kann.

\textbf{Auswertung:}

Die Ergebnisse belegen, dass eine strengere Wahl von \(\epsilon\) (hier
\(\epsilon = 10^{-10}\)) zwar rechnerisch aufwendiger ist, jedoch eine
verlässlichere Konvergenz ermöglicht. Bei einem zu großen \(\epsilon\) kann das
Verfahren frühzeitig stoppen, bevor eine hinreichende Genauigkeit erreicht
wird. Die Auffälligkeit für \(n = 4\) zeigt, dass bestimmte Gitterfeinheiten
unerwartet genaue Resultate liefern können. Insgesamt motiviert dies eine
weiterführende Untersuchung, um eine optimale Wahl von \(\epsilon\) zu
bestimmen.

\subsection{Optimale Wahl von \(\epsilon\)}\label{sec:optimal_epsilon}

In den vorangegangenen Experimenten zeigte sich, dass die Wahl von \(\epsilon\)
einen erheblichen Einfluss auf die Konvergenz des SOR-Verfahrens ausübt. Um den
Einfluss dieser Größe weiter zu untersuchen, betrachten wir nun, wie sich der
maximale absolute Fehler bei unterschiedlicher Skalierung von \(\epsilon\)
entwickelt. Hierzu setzen wir
\[
    \epsilon = h^k \quad \text{mit} \quad k \in \{ -2, 0, 2, 4, 6 \}.
\]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{optimal_eps_plot}
    \caption{Maximaler absoluter Fehler für verschiedene \(\epsilon\)}
    \label{fig:optimal_epsilon}
\end{figure}

\begin{code}
    \captionof{listing}{Code Snippet zum Generieren von \autoref{fig:optimal_epsilon}}
    \label{code:optimal_epsilon}
    \inputminted[firstline=19, lastline=24]{bash}{figures/generate.sh}
\end{code}

\textbf{Beobachtung:}

Mit wachsendem Exponenten \(k\) nimmt der maximale Fehler systematisch ab. Bei
\(k \in \{-2, 0\}\) zeigt sich ein ähnlicher Verlauf: Der Fehler sinkt bis etwa
\(n \approx 100\), steigt anschließend jedoch erneut an. Für \(k = 2\) verläuft
die Kurve insgesamt niedriger, flacht aber etwa ab \(n \approx 400\) ab. Für
\(k = 4\) und \(k = 6\) sind indes kaum noch Unterschiede erkennbar, da beide
Verläufe nahezu identisch sind.

\textbf{Auswertung:}

Diese Ergebnisse legen nahe, dass ein zu kleines \(\epsilon\) ein vorzeitiges
Abbrechen der Iterationen bewirkt, während ein zu großes \(\epsilon\) zu
unnötig vielen Iterationen führt. Der Umstand, dass die Verläufe für \(\epsilon
= h^{-4}\) und \(\epsilon = h^{-6}\) praktisch übereinstimmen, deutet darauf
hin, dass \(\epsilon = h^{-4}\) eine sinnvolle Wahl darstellt: Eine weitere
Absenkung von \(\epsilon\) bringt keinen zusätzlichen Gewinn an Genauigkeit.

\subsection{Optimaler Relaxationsparameter \(\omega\)}

In den vorangegangenen Experimenten wurde der Einfluss des
Relaxationsparameters \(\omega\) weitgehend vernachlässigt, indem sein Wert
konstant auf 1.5 festgelegt wurde. In diesem Abschnitt untersuchen wir nun
gezielt, wie sich unterschiedliche Werte von \(\omega\) auf die Konvergenz des
SOR-Verfahrens auswirken.

Um den optimalen Wert für \(\omega\) experimentel zu bestimmen, haben wir das
SOR-Verfahren bei verschiedenen Gitterfeinheiten \(n\) und
Relaxationsparametern \(\omega\) angewendet. Die Abbildung
\autoref{fig:optimal_omega} zeigt den daraus resultierenden Fehler in
Abhängigkeit von \(\omega\) und \(n\).

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{optimal_omega_plot}
    \caption{Fehler in Abhängigkeit von \(\omega\) und der Gitterfeinheit \(n\)}
    \label{fig:optimal_omega}
\end{figure}

\begin{code}
    \captionof{listing}{Code Snippet zum Generieren von \autoref{fig:optimal_omega}}
    \label{code:optimal_omega}
    \inputminted[firstline=26, lastline=30]{bash}{figures/generate.sh}
\end{code}

\textbf{Beobachtung:}

Der 3D-Oberflächenplot in \autoref{fig:optimal_omega} illustriert deutlich,
dass der logarithmische Fehler bei variierenden Werten von \(\omega\) und \(n\)
stark schwankt. Zusätzlich verdeutlicht der 2D-Plot auf der rechten Seite,
welche \(\omega\)-Werte für die jeweiligen Gitterfeinheiten empirisch als
optimal ermittelt wurden. Diese empirischen Werte (rote Punkte) stehen im engen
Zusammenhang mit den theoretisch hergeleiteten optimalen Werten omegas
(\(\omega_{opt}\) aus \autoref{eq:omega_opt}), was durch die blaue gestrichelte
Linie im Diagramm dargestellt ist.

\textbf{Auswertung:}

Die Analyse verdeutlicht, dass \(\omega\) maßgeblich an der
Konvergenzgeschwindigkeit des SOR-Verfahrens beteiligt ist. Insbesondere hängt
der optimale Wert von \(\omega\) wesentlich von der Größe des Gitters ab: Für
kleinere Gitterfeinheiten (d.\,h. größere Werte von \(n\)) nähert sich der
empirische Optimumswert dem theoretischen \(\omega_{opt}\). Somit bestätigen
die Messdaten die theoretischen Vorhersagen.

Die Wahl des optimalen Relaxationsparameters \(\omega_{opt}\) stellt sich somit
als hocheffiziente Strategie heraus, um sowohl die Konvergenzrate als auch den
Rechenaufwand zu optimieren. Dieser Aspekt gewinnt insbesondere für große
Gitterfeinheiten an Bedeutung, da hier die Anzahl erforderlicher Iterationen
und damit die Gesamtrechenzeit erheblich reduziert werden können.

In diesem bericht werden ab sofort alle nachfolgenden Experimente unter
Verwendung des optimalen Relaxationsparameters \(\omega_{opt}\) durchgeführt.

\subsection{Vergleich mit dem LU-Verfahren}

Da wir bereits das LU-Verfahren zu Lösung des Poisson-Problems untersucht
haben, scheint es sinnvoll, die beiden Verfahren miteinander zu vergleichen.

\subsubsection{Konvergenzverhalten}

In diesem Abschnitt betrachten wir, inwiefern beide Verfahren in der Lage sind,
die exakte Lösung zu approximieren. Zu diesem Zweck wurde erneut der maximale
absolute Fehler in Abhängigkeit der Iterationen analysiert. Für das
SOR-Verfahren haben wir sowohl $\epsilon = 10^{-3}$ als auch den im Abschnitt
\ref{sec:optimal_epsilon} ermittelten optimalen Wert von \(\epsilon\)
verwendet.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{comparison_plot}
    \caption{Vergleich des maximalen absoluten Fehlers für das LU- und
        SOR-Verfahren}
    \label{fig:convergence-comparison}
\end{figure}

\begin{code}
    \captionof{listing}{Code Snippet zum Generieren von \autoref{fig:convergence-comparison}}
    \label{code:covnergence-comparison}
    \inputminted[firstline=32, lastline=36]{bash}{figures/generate.sh}
\end{code}

Wie in Abbildung \ref{fig:convergence-comparison} ersichtlich, ähneln sich die
Verläufe beider Verfahren bis etwa \(N \approx 3000\). Ab diesem Punkt steigt
der maximale Fehler beim SOR-Verfahren mit \(\epsilon = 10^{-3}\) wieder an.
Der für das LU-Verfahren und das SOR-Verfahren mit optimalem \(\epsilon\)
ermittelte Fehler bleibt hingegen konstant konvergent. Hierbei ist zu beachten,
dass die Rechnungen mit dem LU-Verfahren aufgrund hoher Rechenzeit nur bis \(n
= 128\) durchgeführt wurden.

\textbf{Auswertung:}
Die Ergebnisse zeigen, dass das LU-Verfahren für moderate Problemgrößen (bis
etwa \(N = 3000\)) gut mit dem SOR-Verfahren konkurrieren kann. Allerdings
erfüllt das SOR-Verfahren bei zu groß gewähltem Abbruchkriterium \(\epsilon\)
die Konvergenzanforderungen nicht mehr zuverlässig. Bei Verwendung eines
sensibler gewählten (optimalen) \(\epsilon\) beweist sich das SOR-Verfahren
hingegen auch für größere Gitter als stabil und konvergent.

\subsubsection{Laufzeitverhalten}

Ein wesentlicher Faktor bei der Wahl des Verfahrens ist dessen Laufzeit. Um
dies zu untersuchen, wurde die Ausführungszeit beider Verfahren in Abhängigkeit
der Gitterfeinheit \(n\) gemessen. Das SOR-Verfahren kam dabei mit optimalem
\(\epsilon\) sowie optimalem \(\omega\) zum Einsatz. Die Messwerte stellen
Mittelwerte mehrerer Durchläufe dar, um zufällige Schwankungen zu glätten.
Sowohl bei der LU-Zerlegung (mittels \texttt{lu\_factor} aus \texttt{scipy})
als auch beim SOR-Verfahren (Sparse Forward-/Backward-Solve) wurde auf eine
möglichst effiziente Implementierung geachtet.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{runtime_comparison_plot}
    \caption{Vergleich der Laufzeiten des LU- und SOR-Verfahrens}
    \label{fig:runtime-comparison}
\end{figure}

\begin{code}
    \captionof{listing}{Code Snippet zum Generieren von \autoref{fig:runtime-comparison}}
    \label{code:runtime-comparison}
    \inputminted[firstline=38, lastline=42]{bash}{figures/generate.sh}
\end{code}

Für kleine Gitterfeinheiten \(n\) zeigt sich, dass die LU-Zerlegung anfangs
geringere Laufzeiten aufweist als das SOR-Verfahren. Ab etwa \(n \approx 35\)
steigt die Laufzeit der LU-Zerlegung jedoch stark an und übertrifft von da an
klar diejenige des SOR-Verfahrens.

\textbf{Auswertung:}
Die Untersuchungen deuten darauf hin, dass das LU-Verfahren sich für kleine
Systemgrößen (z.\,B.\ bis \(n \sim 30\)) lohnt, da hier die direkte
Faktorisierung vergleichsweise schnelle Resultate liefert. Für größere
Problemgrößen wächst der Rechenaufwand jedoch massiv, was das iterativ
arbeitende SOR-Verfahren (mit optimierten Parametern) klar bevorzugt erscheinen
lässt. Gerade bei hoch aufgelösten Gittern erweist sich die Sparsity der
Matrix \(A\) als vorteilhaft für SOR, während die LU-Zerlegung in solchen
Fällen typischerweise teure und dichte Faktoren erzeugt.

\section{Zusammenfassung}

In diesem Bericht wurde das Poisson-Problem unter Verwendung des
Successive-Over-Relaxation-Verfahrens (SOR) untersucht und mit einer
LU-Faktorisierung verglichen. Die Experimente zeigen, dass das SOR-Verfahren
insbesondere für größere Gittergrößen klar im Vorteil ist, da es stark von der
Sparsity der Matrix profitiert. Der optimale Relaxationsparameter \(\omega\)
und ein geschicktes Abbruchkriterium \(\epsilon\) (etwa \(\epsilon = h^{-4}\))
erweisen sich dabei als entscheidende Faktoren für eine effiziente und stabile
Konvergenz. Im Gegensatz dazu wird die LU-Zerlegung mit steigender Problemgröße
schnell rechenaufwendig, da bei dichter Faktorstruktur die Speicher- und
Rechenkosten stark ansteigen.

Die Ergebnisse unterstreichen die Bedeutung gut abgestimmter iterativer
Verfahren bei der Lösung dünnbesetzter linearer Systeme aus der Diskretisierung
partieller Differentialgleichungen. Während ein direkter Löser wie LU für
kleinere Probleme vorteilhaft sein kann, dominiert das iterative SOR-Verfahren
ab mittleren bis großen Gittergrößen hinsichtlich Ress

\printbibliography

\end{document}
